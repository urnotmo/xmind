# 人工智能

## 目标检测技术

### 介绍

- 在一间乱七八糟的房间里，你花了多少时间寻找丢失的钥匙？如果一个简单的计算机算法能在几毫秒内找到你的钥匙呢？
- 这就是目标检测的力量。对象检测的应用跨越了多个不同的行业，从24小时实时监控到智能城市中的实时车辆检测...
- 对象检测的各种算法：

	- 1）DPM
	- 2）RCNN家族：RCNN, Fast RCNN和Faster RCNN
	- 3）YOLO
	- 4）SSD
	- 5）RetinaNet
	- 6）MTCNN——多个"单目标"检测。

### 1.RCNN家族

- 前提

	- 如何用CNN来解决一般的目标检测问题？

		- 1）首先，我们取一个图片作为输入。
		- 2）然后，我们将图片划分为不同的区域(通过滑动窗口)。
		- 3）接着，我们将每个区域看作一个单独的图像。
		- 4）将所有这些区域传递(图像)给CNN，并将其划分为不同的类。
		- 5）将每个区域划分为相应的类后，我们可以将这些区域结合起来得到原始图像和检测到的物体。

	- 这种目标检测的缺陷：

		- 不同的对象具有不同的纵横比和空间位置，对象的形状、尺寸也会不同。由于这些因素，我们需要非常多的区域，从而导致大量的计算时间。

	- 为了解决这个问题，减少区域的数量，诞生了region-based CNN——使用建议方法(如: selective search算法)来提取这些候选区域(region proposal)。

- RCNN

	- RCNN中selective search工作原理简述

		- 一个物体有4个基本的部分组成：varying scales, colors, textures, and enclosure(不同的尺寸、颜色、纹理、外壳/形状)。
		- 1）首先，以一个图像作为输入；
		- 2）然后，它生成初始的子分割，从图像中得到大量的区域；

		- 3）接着，该技术结合相似区域形成更大的区域(基于颜色相似度、纹理相似度、尺寸相似度、形状兼容)；

		- 4）最后，这些区域会生成最终的目标位置(感兴趣的区域，ROI，region of interest)。

	- 结构

	- RCNN检测对象步骤简述

		- 1）首先，使用一个预先训练好(pre-trained)的卷积神经网络。
		- 2）然后，对该模型进行再训练。根据需要检测的类的数量来训练网络的最后一层。
		- 3）第三步，使用selective search算法获取每张图的感兴趣区域(2000个左右)，然后reshape这些区域以匹配CNN的输入大小(统一缩放成227*227)。
		- 4）接着，训练一个二分类支持向量机(SVM)来分类物体和背景。
		- 5）最后，我们训练一个线性回归模型来为图像的每个识别对象生成更紧密的边界盒。

	- 通过一个可视化的示例，来更好的理解上面的步骤

		- 1）首先，取一幅图像作为输入。
		- 2）然后，使用一些建议方法(如 selective search算法)来提取感兴趣区域(ROI)。
		- 3）根据CNN的输入对这些区域进行reshape，并传递给ConvNet。
		- 4）接着，CNN提取每个区域的特征，并使用SVM对它们进行分类。
		- 5）最后，使用边界框回归(Bbox reg)来预测每个识别区域的边界盒。

	- 缺陷

		- RCNN训练即昂贵又缓慢

			- 1）基于selective search为每张图片提取2000个左右的region proposal。
			- 2）利用CNN对每个图像区域提取特征。假设我们有N张图片，那么CNN的特征数将是N*2000。
			- 3）使用RCNN进行目标检测的整个过程包含三个模型：

				- 1.用于特征提取的CNN。
				- 2.用于识别对象的线性SVM分类器。
				- 3.用于收紧边框的回归模型。

		- 对每幅新图像进行预测需要40-50秒，使得模型在面对巨大的数据集时变得非常繁琐。

- Fast RCNN

	- 前提

		- RCNN存在诸多局限性，如何减少RCNN的计算时间？
		- 解决思路：每张图片只运行一次CNN(代替原来的2000次)，并找到一种方法在2000个区域共享计算结果。

	- 结构

	- Fast RCNN步骤简述

		- 1）首先，将图像作为输入。
		- 2）然后，由ConvNet提取特征得到feature map，使用一种建议方法(selective search)从feature map中提取候选区域。
		- 3）接着，应用ROI池化层确保所有的区域大小一致。

			- 空间金字塔池化 或 自适应池化

		- 4）最后，这些区域被传递给一个全连接网络(多层)，由全连接网络对它们进行分类(Linear+softmax)和边界框回归(Linear)。

	- Fast RCNN解决了RCNN的两个主要问题：

		- 1）每张图只向ConvNet传递一次而不是2000个区域。
		- 2）使用一个模型而不是三个不同的模型来提取特征、分类、边界框回归。

	- 缺陷

		- 1）寻找感兴趣区域的建议方法仍然使用selective search算法，这是一个缓慢而费时的过程。
		- 每幅图像检测物体大约需要2秒，尽管比RCNN好得多，但考虑到现实中的大型数据集时，Fast RCNN也显得很慢。

- Faster RCNN

	- 论文

		- Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
		- https://arxiv.org/abs/1506.01497

	- Faster RCNN是Fast RCNN的修改版，它们的主要不同在于：

		- Fast RCNN：使用Selective Search来生成感兴趣区域。
		- Faster RCNN：使用RPN(Region Proposal Network)来生成感兴趣区域。

	- 结构

	- Faster RCNN步骤

		- 1）将一个图像作为输入，传递给ConvNet，得到一个图像feature map。
		- 2）feature map经由RPN(候选区域生成网络)得到候选框和得分(是一个对象的概率)，然后再对feature map进行裁剪。
		- 3）应用ROI池化层确保所有的区域大小一致。
		- 4）最后，通过一个全连接层(顶部是一个softmax层和一个线性回归层)来进行分类和边界框回归。

	- Faster RCNN的核心是RPN，RPN的工作原理简述：

		- 首先，Faster RCNN从CNN获取特征图，并传递给Region Proposal Network。
		- RPN在这些特征图上使用一个滑动窗口，在每个窗口中，它生成k个不同形状和大小的锚框(Anchor boxes)。
		- 锚框是固定大小的边界框，它被放置在整个图像中，具有不同的形状和大小。对于每个锚框，RPN做了两件事情：

			- 1）锚框是一个对象的概率(不考虑该对象属于哪一个类)。
			- 2）调整锚框以更好的匹配对象。

		- 不同形状和大小的边界框被传递到ROI池化层，提取固定大小的feature maps。
		- 然后，feature maps被传递到一个全连接层(包含一个softmax和一个线性回归层)，对目标进行分类和边界框回归。

	- Faster RCNN源码中Anchor的计算

		- https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/rpn/generate_anchors.py

		- 初始参数base_size=16, ratios=[0.5, 1, 2], scales=2**np.arange(3, 6)

			- base_size用来生成初始锚框[0, 0, base_size-1, base_size-1]。
			- ratios表示锚框的高宽比。由ratios=h/w，s=h*w -->w=sqrt(s/ratios)，h=w*ratios。
			- scales是锚框的缩放倍数2**3=8,2**4=16,2**5=32。

		- 1）生成初始锚框base_anchor

			- base_anchor = np.array([1, 1, base_size, base_size]) - 1

		- 2）根据不同的高宽比ratios=[0.5, 1, 2]，得到三个不同的ratio_anchors(面积与base_anchor一致)

			-     w, h, x_ctr, y_ctr = _whctrs(base_anchor)
    size = w * h
    size_ratios = size / ratios
    ws = np.round(np.sqrt(size_ratios))
    hs = np.round(ws * ratios)
    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)

		- 3）对于每个ratio_anchor，都根据缩放比例分别得到三个不同尺寸的anchors，最后一共得到9个anchors。

			-     w, h, x_ctr, y_ctr = _whctrs(ratio_anchor)
    ws = w * scales
    hs = h * scales
    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)

	- Faster RCNN论文中的标签

		- (xa, ya)是anchor的中心，wa、ha是anchor的宽高。

	- 缺陷

		- 1）该算法需要对一幅图像进行多次遍历才能提取出所有的目标。
		- 2）由于不同的系统一个接一个地工作，因此后面系统的性能取决于前一个系统的性能。

	- RCNN家族的时间对比

- 总结

	- CNN

		- 特点: 将图像划分为大量的区域，然后对每个区域进行分类。

			- 耗时: ---

				- 缺陷: 需要对大量区域进行精确预测，因此计算时间很长。

	- RCNN

		- 特点: 使用selective search生成候选区域(约2000个)。

			- 耗时: 40-50秒

				- 缺陷: 1）每个区域分别传递给CNN，计算时间长。2）使用三种不同的模型进行预测。

	- Fast RCNN

		- 特点: 1）每张图只传递给CNN一次，在提取的feature maps上使用selective search生成候选区域。2）将三种模型结合为一个模型。

			- 耗时: 2秒

				- 缺陷: selective search很慢，因此计算时间仍然很高。

	- Faster RCNN

		- 特点: 使用RPN代替selective search生成候选区域使得算法更加快速。

			- 耗时: 0.2秒

				- 缺陷:

					- 1）多次遍历图像才能提取所有的目标(对于每个滑动窗口，都生成9个锚框)。
					- 2）不同系统一个接一个工作，后面系统的性能取决于一个系统的性能。

### 2.YOLO系列

- YOLO官网

	- https://pjreddie.com

- 1.YOLOv1

	- 论文

		- You Only Look Once: Unified, Real-Time Object Detection
		- https://arxiv.org/abs/1506.02640

	- 核心思想

		- 一个简单的神经网络通过对完整图片的一次检验，直接预测出边界框和分类类别。
		- Faster RCNN也是用整张图片作为输入，但是使用RPN(Region Proposal Net)多次遍历才能提取所有的目标。

	- 应用技术

		- 1）非极大值抑制

			- 非极大值抑制

				- 目标检测算法最常见的问题之一：对同一个目标检测多次而不是一次。
				- 非极大值抑制解决了这个问题，让我们来看下它是如何工作的？

					- 1）首先，选取概率最大的框。
					- 2）然后，抑制与该框有着高IOU的框。
					- 3）在剩余的框中，重复1）、2）的过程，直到所有的框都被选中。

			- IOU

				- IoU = Area of the intersection / Area of the union
				- 如果IoU大于0.5(阈值，可更改)，我们就可以说预测足够好了。

	- 大致流程

		- 1）将输入图片分成S*S的网格(YOLOv1还没有区分大中小三种尺寸)。

			- 如果物体的中心点落在某一个网格中，这个网格单元将负责识别出该物体。

		- 2）对于每个grid，预测B个边界框(x,y,w,h)和置信度c。

			- 坐标(x, y)是相对当前网格左上角的偏移量除以cell_size归一化到[0, 1]之间。

				- github上有人除以整个图片的宽、高做归一化，但效果没有直接除以cell_size的效果好。

			- w，h是实际框的宽高用整个图像的width和height归一化到[0, 1]之间。
			- c是对象落在某预测框中的置信度。

				- 置信度为=Pr(Object)∗IOUtruthpred，如果该网格中没有对象，则置信度为0；否则，置信度等于预测框与实际框的IOU。
				- 置信度反应该网格中是否有对象以及预测框与实际框的契合程度。
				- 疑问？

				  ？

					- 我们知道，YOLOv2的两个Anchor box可以预测同一个grid中的两个对象。但早期的YOLOv1一个网格最多只能检测一个对象，那么为什么要用两个bounding box？
					- 答：目的是为了提高精度。使用一个bounding box来对所有形状的物体做分类识别，精度记为A；使用两个bounding box，第一bounding box专门用来做横长方形形状物体的分类识别，第二bounding box专门用来做纵长方形形状物体的分类识别，精度记为B。B的精度比A高。

			- 注意：若该grid中有对象，那么预测框与实际框的IOU最大的框负责识别该对象。

		- 3）对于每个grid，同时也预测了属于C类别的条件概率，Pr(Classi|Object)。

			- 在test的时候，每个网格预测的类条件概率与置信度得到每个框的对应类别置信度：

			- 只预测每个网格单元的类概率集，而不考虑boxes B的数量。标签形如：

		- 4）最后，对于每一类，先根据类置信度阈值过滤一部分，然后再根据nms过滤bbox。

	- 模型设计

		- 借鉴了 GoogLeNet。24个卷积层+2个全连接层。卷积层从图像中提取特征，全连接层预测输出概率和坐标。

	- 训练

		- 1）最后一层使用线性激活函数，其他层都使用Leaky ReLU激活函数。
		- 2）使用均方和误差来计算所有损失，因为它容易优化(然后，它并不完全符合我们最大化平均精度的目标)。
		- 3）定位(边框回归)误差与分类误差权重一致(这可能并不理想)。

			- 优化：增加边框回归损失的权重，λcoord= 5。

		- 4）noobj的置信度为0，且占大多数，会导致模型不稳定。

			- 优化：减少noobj的置信度权重，λnoobj=0.5。

		- 5）均方和误差对于大框和小框的误差权重也是一样的(但我们的误差指标应该反应小的偏差对小框的影响比大框更大)。

			- 部分解决这个问题：预测w和h的平方根来替代直接预测w和h。

		- 6）预测每个grid的边界框有多个，我们指定预测框与实际框的IOU最大的那个对应边界框来负责预测对象。
		- 损失函数

				- 表示第i个网格中有对象。

					- 只有当某个网格中有object的时候才对classification error进行惩罚。

				- 表示第i个网格中含有对象且第j个bbox是负责预测的。

					- 只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚。

	- 优缺点

		- 优点

			- 1）YOLO速度快，能够达到实时的要求。

				- 区别于RCNN对图片多次遍历获取候选区域，YOLO对完整图片只检验一次。
				- 在 Titan X 的 GPU 上能够达到 45 帧每秒。

			- 2）YOLO使用全图作为Context(上下文)信息，背景错误（把背景错认为物体）比较少。

				- YOLO产生的背景识别错误不到R-CNN的一半。

			- 3）YOLO泛化能力强。

				- 在自然图像进行训练并且在艺术品上进行测试时，YOLO的表现远远超过DPM和R-CNN等顶级的检测方法。

		- 缺点

			- 1）边界框回归对大框和小框的惩罚力度一样，导致较高的定位错误。

				- 由于相同的偏差对小框的影响比大框的印象更大，所以正确的做法应该对小框有更大的惩罚力度。

			- 2）每个网格只能预测一个类，对小物体和密集物体的识别精度低。

- 2.YOLOv2/YOLO9000

	- 论文

		- YOLO9000: Better, Faster, Stronger
		- https://arxiv.org/abs/1612.08242

	- 改进YOLOv1

		- 1）YOLOv1产生了大量的定位错误。
		- 2）与proposal方法生成候选区域相比，YOLOv1的召回率较低。
		- 因此，YOLO9000主要关注改善召回率和定位，同时保持分类准确性。

	- 应用技术

		- 1）非极大值抑制nms
		- 2）锚框

			- 锚框是另一种提高YOLO算法性能的方法。YOLOv2通过锚框来解决一个网格中有多个对象的情况，提升召回率。
			- YOLOv1中每个网格只能识别一个对象，但如果一个网格中有多个对象呢？现实中经常出现这种情况，这就引出了锚框的概念。
			- Anchor Boxes原理简述

				- 如右侧所示，同一个网格中同时有两个对象：car或person。

				- 首先，预定义两个不同形状的锚框，输出由3*3*8变成3*3*16.

				- 然后，不同的对象根据边界框与锚框形状的相似性分配给不同的锚框。

					- 由于锚框1的形状与person的边界框相似，因此person将被分配到锚框1，car将被分配到锚框2。

				- 如此，对于每个网格，可以通过锚框的数量检测两个或更多的目标。

	- 优化策略

		- YOLOv2主要集中在：保持分类精度的同时，改善YOLOv1的低召回率和高定位误差。
		- 1）Batch Normalization，每个卷积层后添加BN，mAP提升2%。
		- 2）High Resolution Classifier，提升输入图片的分辨率(到448 * 448)，mAP获得了4%的提升。
		- 3）引入Anchor Boxes，一个网格可预测多个类。

			- 可以预料到的结果是召回率上升，准确率下降。

				- 没有anchor boxes，模型recall为81%，mAP为69.5%；
				- 加入anchor boxes，模型recall为88%，mAP为69.2%。
				- 准确率只有小幅度的下降，而召回率则提升了7%。

		- 4）聚类提取Anchor Boxes先验框。

			- 问题

				- 引入Anchor Boxes后的第一个问题：锚框的尺寸是手动挑选的，如果一开就为网络选择更好的先验框，可以让网络更容易学习到更好的结果。

			- 优化策略：使用K-means聚类提取更具有代表性的Anchor Boxes。
			- 优化步骤

				- 1）传统的K-means聚类方法使用的是欧氏距离函数，对大框和小框惩罚力度一致，聚类结果可能会偏离。因为小的偏差对小框的影响比大框更大，所以应该对小框有更大的惩罚。
				- 2）采用IOU作为判断标准，这样就与box的尺寸无关，改善后的距离函数: d(box,centroid) = 1−IOU(box,centroid)
				- 3）在召回率和模型复杂度之间进行平衡之后，取了k=5。

			- 优化结果：5个anchor boxes的召回率就和Faster R-CNN的9种相当。

		- 5）直接位置预测(Direct location prediction)。

			- 问题

				- 采用anchor boxes的第二个问题：tx,ty没有限制，使模型不稳定，尤其是在早期迭代过程中。

					- RPN网络中预测公式：

					- (xa, ya)是anchor中心点坐标，(x, y)是预测框中心点坐标。
					- 由于ancho可以在图像中的任意位置，tx,ty取值没有限制，从而导致模型不稳定。

			- 优化

				- 1）沿用YOLOv1的方法，tx,ty是相对当前网格(并除以cell_size归一化)的偏移量，但使用Sigmoid激活函数保证反算时tx´、ty´的值为正。
				- 2）w、h和Faster RCNN一样取log，保证反算时w´、h´为正。
				- 3）Pr(object)∗IOU(b,object) =σ(to)，YOLOv1是直接用to，但这里对to做sigmoid变换，保证反算时的置信度为正。

		- 6）细粒度特征。为了尽可能保留较小对象的特征，添加passthrough层使mAP提升1%。

			- 在最后一次池化之前，先将原特征图分成4分，然后与池化后的特征图叠加在一起作为输出的特征图。

		- 7）多尺度训练

			- 去掉全连接层，可以接受各种尺寸(32的倍数即可)。用不同的尺寸进行训练，可使模型对各种尺度的图片检测都又具有稳健性，提高了泛化能力。

		- 8）高分辨率的对象检测。模型支持不同尺寸的输入，分辨率从416*416换为544*544，mAP可提升1.8%。

	- 模型

- 3.YOLOv3

	- 论文

		- YOLOv3: An Incremental Improvement
		- https://arxiv.org/abs/1804.02767

	- 集百家之所长，进行优化。

		- 1）边界框预测

			- 1.仍然是聚类生成Anchor boxes，由k=5变为k=9。
			- 2.边框损失(tx，ty，tw，th)使用平方和误差来计算。

			- 3.改进: 用logistic回归(二分类)来预测边框的objectness score。

				- 1）我们只取与GT的IOU最大的那个先验框来预测对象，其objectness score值为1。
				- 2）如果一个先验框不负责预测对象，它只用来计算objectness损失(不计算边框损失和分类损失)。

		- 2）分类预测

			- 改进: 多标签方法，使用单独的二分类器来进行多类别预测。

				- 什么不用softmax？-->有助于把YOLO用于更复杂的领域类似，如包含了大量重叠的标签(如女性和人)的Open Images dataset数据集。

		- 3）backbone(主干网络)——Darknet-53

		- 4）passthrough layer改为残差结构，先上采样再与浅层特征融合，有助于保留细粒度特征，使检测小目标的效果更好。

	- 结构

	- 损失函数

			- 1）当前网格中有对象并且某边框负责预测对象时，标记为obj，用来计算边框回归损失和分类损失。

				- 边框回归用MSE
				- 分类用BCE

			- 2）否则，先全部标记为noobj。然后在noobj中，将IOU>0.5(ignore_thres)的标记为-1，表示忽略该预测。
			- 3）obj和noobj用来计算置信度损失。

				- 置信度用分类用BCE

			- 优化

				- 1）增加边框回归损失的权重，λcoord= 5。
				- 2）减少noobj的置信度权重，λnoobj=0.5。
				- 视情况而定，非绝对。github上的PyTorch-YOLOv3是λcoord=1，λnoobj=100。

	- YOLOv3 vs YOLOv2

		- 1）K由5变成9，增加了单个gird的目标检测数量。
		- 2）替换softmax loss为logistic loss，且每个GT只匹配一个先验框。
		- 3）一个detection变成3个：13*13，26*26，52*52。并引入残差结构，上采样特征与浅层特征融合在一起有助于保留细粒度特征，检测小目标的效果更好。
		- 4）darknet-19变成darknet-53。

	- 补充

		- YOLOv3-Tiny

- YOLOv4

	- 结构

	- 哪些优化策略？

		- 激活函数优化

			- YOLOv4中使用了Mish激活函数

			- 顺便了解下Swish激活函数

				- 保留Sigmoid的强非线性能力的同时，克服Sigmoid两侧梯度消失的问题。

					- Sigmoid的缺点：饱和函数，易导致梯度消失。
					- ReLU的缺点：非线性能力弱。

		- 损失优化

			- BBOX边框回归采用CIOU_LOSS，提升了预测框回归的速度和精度。
			- 了解下回归损失发展史

				- (L1+L2)-->Smooth L1 Loss --> IoU Loss --> GIoU Loss --> DIoU Loss --.>CIoU Loss
				- 1）使用SMOOTH_L1损失函数

					- SMOOTH_L1损失避开了L1和L2损失的缺陷，结合了MAE和MSE的优点。
					- L1损失(MAE)

						- 优点：比较稳健(鲁棒)。两侧梯度不变，不易导致梯度爆炸。
						- 缺点：不稳定解。中间折点不可导，不方便求解。

					- L2损失(MSE)

						- 优点：稳定解。各点都连续光滑可导，具有较为稳定的解。
						- 缺点：不稳健(鲁棒性较差)。两侧梯度较大，易导致梯度爆炸。

				- 2）IOU_LOSS系列

					- IOU_LOSS

						- 存在的问题：

							- 1）两框不重叠的情况，无法区分好坏。
							- 2）两框相交的情况，无法区分好坏。

					- GIOU_LOSS

						- GIOU_LOSS仍然存在的问题：框内嵌套的情况，无法区分好坏。

					- DIOU_LOSS

						- DIOU_LOSS仍然存在的问题：当IOU和中心点距离相同时，无法区分哪种形状更好。

					- CIOU_LOSS

					- 总结：

						- IOU_LOSS：主要考虑检测框和目标框重叠面积。
						- GIOU_LOSS：在IOU的基础上，解决边界框不重合时的问题。
						- DIOU_LOSS：在IOU和GIOU的基础上，考虑边界框中心点距离的信息。
						- CIOU_LOSS：在DIOU的基础上，考虑边界框宽高比(形状)的尺寸信息。

		- 训练优化

			- 1.数据预处理

				- pixel-wise(像素级)调整

					- 1）几何畸变

						- 缩放
						- 剪切
						- 翻转
						- 旋转

					- 2）光度畸变

						- 亮度
						- 对比度
						- 饱和度
						- 色调
						- 添(加)噪声

				- 遮挡

					- Random Erase，CutOut：随机选择图像中的矩形区域，并填充一个随机的或互补的零值。

						- Random Erase与CutOut的不同之处：

							- 1）选择的方形大小不是固定的；
							- 2）填充区域选择的是[0, 255]的随机值，而不是0;
							- 3）方形区域限制在图像内。

					- hide-and-seek、grid mask：随机或均匀地选择图像中的多个矩形区域，并将其全部替换为0。
					- feature map上做"正则"处理：

						- Dropout：整体随机扔。

						- DropConnect：只在连接处扔，神经元不扔。
						- Spatial Dropout：按通道随机扔。
						- DropBlock：每个特征图上按spatial块随机扔。

				- 结合多张图来进行增强

					- Mixup
					- CutMix

				- GAN

					- Style Transfer GAN

				- 解决类别不平衡问题

					- 两个阶段

						- Hard Negative Example Mining(困难负样本挖掘)
						- Online Hard Example Mining(在线困难样本挖掘)

					- 单阶段

						- Focal Loss(目标是解决样本类别不平衡以及样本分类难度不平衡等问题)

				- one-hot类别之间没有联系

					- Label Smoothing(标签平滑)
					- 知识蒸馏

			- 2.后处理方法

				- 用DIOU-NMS替代NMS

					- NMS: 非极大值抑制
					- soft-NMS: 解决对象的遮挡问题
					- DIou-NMS: 将中心点分布信息添加到BBox筛选过程中

	- YOLOv4核心

		- 1.最终方案

			- YOLOv4模型 = CSPDarkNet53 + SPP + PANet(path-aggregation neck) + YOLOv3-head

				- backbone：CSPDarknet53[81]
				- neck：SPP[25]，PAN[49]
				- head：YOLOv3[63]

			- 主干线免费包（BoF）：CutMix和Mosaic数据增强、DropBlock正则化、类标签平滑
			- 主干网专用包（BO）：误激活、跨阶段部分连接（CSP(Cross-Stage-Partial-connection)）、多输入加权剩余连接（MiWRC）
			- 检测的免费包（BoF）：CIoU丢失，CmBN，DropBlock正则化，马赛克数据增强，自我对抗训练，消除网格敏感度，使用多个锚实现单一地面真理，cosineanAlingScheduler[52]，最佳超参数，随机训练形状。
			- 检测专用包（BoS）：Mish激活、SPP块、SAM块、泛路径聚合块、DIoU NMS Experiments

		- 2.创新点

			- 1）提出新型数据增强方法 Mosaic 和自对抗训练（SAT(Self adversarial training)）；
			- 2）修改现有方法，使新方法实现高效训练和检测：modified SAM、modified PAN 和 CmBN

				- (1)modified SAM：将pooling层改为了卷积层。
				- (2)modified PAN：在连接处将addition改为concatenation。
				- (3)CmBN(Cross mini-Batch Normalization)：CmBN-使用跨小批量规范化来收集整个批次内的统计信息，而不是在单个小批量内收集统计信息。

- YOLOv5

	- https://github.com/ultralytics/yolov5.git

	- 总结

		- 1.V4相对V3，性能差不多，但精度有多提升。-->提升精度
		- 2.V5比V4、V3性能好很多，精度与V4差不多。-->进一步提升性能

### 对比YOLO vs Faster-RCNN

- 1）YOLO比fast - rcnn快。
- 2）YOLO不适用于小物件。

	- 如果你想提高它在较小物件上的性能，可以这样修改：1）增加锚框的数量 2）降低IOU的threshold。

### YOLOv3项目总结

- 代码github地址

	- https://github.com/lyf524951805/YOLO/tree/master/YOLOv3-PyTorch

- 项目流程

	- 一、数据、配置文件初始化

		- 1.下载VOC2012数据集，百度网盘：
        链接：https://pan.baidu.com/s/1YnvZiTY-Z7VrHMMlMmep7w
        提取码：tyd0
		- 2.修改config/voc2012.data文件：
        root为本地VOC2012数据集目录，
        sample为中间样本(416*416)的保存目录，
        names为VOC2012类文件，
		- 3.执行gen_sample.py，生成中间样本:
        1）生成VOC2012类文件，并将CLASS_NUM写入constant.py文件中，
        2）生成中间样本(416*416)，划分训练集:验证集=4:1，
        3）同时将中间样本的标签文件train_label_path、val_label_path写入config/voc2012.data文件中，

        样本生成成功后，可调用verify_mergy_label()验证生成标签的准确性。
		- 4.执行gen_anchors.py，生成锚框:
        1）使用自定义的kmeans聚类(d(box,centroid) = 1−IOU(box,centroid))生成锚框，并写入到constant.py文件中

	- 二、训练

		- 执行train.py进行训练，自定义如下参数：
        --data_config
        --img_size
        --pretrained_weights
        --epochs
        --batch_size
        --n_cpu
        --gradient_accumulations

	- 三、检测

		- 执行detect.py进行检测，自定义如下参数：
        --image_folder
        --weights_path
        --class_path
        --conf_thres
        --nms_thres
        --batch_size
        --n_cpu
        --img_size

- 调参过程

	- 1.第一阶段

		- 在checkpoints和logs在colab/phase1目录下
    # 损失权重系数
    SCALE_BBOX_OBJ = 1
    SCALE_CONF_OBJ = 1  # 正样本置信度损失的权重
    SCALE_CONF_NOOBJ = 100  # 负样本置信度损失的权重(此值越大，模型抗误检能力越强)
    SCALE_CLS_OBJ = 1

    30轮：train_loss: 4.288545231615947 val_loss: 8.393006727134154
    之后train_loss继续下降，val_loss基本不变。

    共训练80轮，训练到50轮训练损失5.086、验证损失19.87，之后训练损失继续下降，但是验证损失基本不变。
    第60轮: 训练损失2.812、验证损失18.32
    第70轮: 训练损失1.733、验证损失22.15
    第80轮: 训练损失1.201、验证损失21.91

    检测结果: SCALE_CONF_NOOBJ的权重太大，虽然抗误检能力强，但是分类和回归精度不高。
        第60轮的检测效果最好。

	- 2.第二阶段

		- 在checkpoints和logs在colab/phase2目录下
    根据第一阶段的检测结果进行参数调整: 减小SCALE_CONF_NOOBJ的权重，以便提升分类和回归的精度。
    # 损失权重系数
    SCALE_BBOX_OBJ = 1
    SCALE_CONF_OBJ = 1  # 正样本置信度损失的权重
    SCALE_CONF_NOOBJ = 10  # 负样本置信度损失的权重(此值越大，模型抗误检能力越强)
    SCALE_CLS_OBJ = 1

    共训练45轮，训练到30轮训练损失4.2885、验证损失8.393，之后训练损失继续下降，但是验证损失基本不变。
    第35轮: 训练损失3.332、验证损失8.978

    检测结果:
        SCALE_CONF_NOOBJ的权重变小了，抗误检能力变弱了。
        分类和回归精度仍然不高。
        第35轮的检测效果最好。

	- 3.第三阶段

		- 进行中...

- 一些注意点

	- 1.anchors的生成:
    kmeans聚类的距离计算公式: d(box,centroid) = 1−IOU(box,centroid)
    方式一: 先将图片转换为正方形，然后resize成(416, 416)，再用聚类生成anchors。
    方拾二: 先将图片转换为正方形，然后对w、h除以正方形的边长做归一化，对归一化后的
        w、h用聚类生成scale_anchors，最后乘以416得到anchors。
	- 2.标签conf置信度的计算:
    大中小尺寸三种，每种各3个锚框，一共9个锚框。
    对于一个对象，各用大中小尺寸中的一个bbox来预测该对象，即9个框中用3个bbox来预测该对象。

    对于小尺寸(大、中尺寸等同)的3个锚框，分别计算对象与锚框的IOU：
        1）初始化所有置信度conf=0，表示noobj；
        2）满足IOU>0.5(ignore_threshold)，置信度conf=-1，表示ignore(忽略该预测)。
        3）取3个中IOU最大的bbox来预测对象，置信度conf=1，表示obj;
	- 3.标签tx,ty,tw,th的计算:
    grid_size表示每个网格的大小，w_anchor和h_anchor是锚框的宽高。
    tx,ty是相对当前网格的偏移:
        tx, x_index = math.modf(cx/grid_size)
        ty, y_index = math.modf(cy/grid_size)
    tw,th是相对锚框的:
        tw = torch.log(w_box/w_anchor)
        th = torch.log(h_box/h_anchor)
	- 4.yolo输出变换:
    1）x，y用sigmoid激活保证值在(0, 1)之间:
        x = torch.sigmoid(x_pred)  # Center x
        y = torch.sigmoid(y_x_pred)  # Center y
        w = w_pred  # Width
        h = h_pred  # Height
    2）conf、cls用sigmoid激活表示概率值，在(0, 1)之间:
        conf = torch.sigmoid(conf_pred)  # Conf
        cls = torch.sigmoid(conf_pred)  # cls
	- 5.损失函数计算:
    1）取obj用来计算边框损失，使用MSE
    2）取obj用来计算分类损失，使用BCE
    3）取obj和noobj用来计算置信度损失，使用BCE
    并通过以下4个系数，来调整不同损失的权重:
        SCALE_BBOX_OBJ
        SCALE_CONF_OBJ
        SCALE_CONF_NOOBJ
        SCALE_CLS_OBJ
	- 6.预测框的非极大值抑制:
    首先，通过置信度阈值(conf_thres=0.8)进行筛选，保留Pr(Object)>=0.8的预测框。
    然后，计算类得分，score=Pr(Classi|Object)∗Pr(Object)。
    接着，根据score对预测框进行排序(从大到小)。
    最后，通过非极大值抑制(nms_thres=0.4)得到最后的预测框。

- 如何继续优化？

	- 1.将voc2012数据集更换成coco数据集，voc2012的数据集太少。
    coco数据集百度网盘：
        链接：https://pan.baidu.com/s/1RzdxpALWz1iNGOoUfJ-Xbg
        提取码：vko7
	- 2.对图片进行增样(翻转)操作，增强泛化性:
    为了方便计算翻转后图片的标签，增样前统一将图片的标签做归一化处理。
    水平翻转示例(target已做归一化处理):
    def horisontal_flip(images, targets):
        images = torch.flip(images, [-1])
        targets[:, 2] = 1 - targets[:, 2]
        return images, targets

