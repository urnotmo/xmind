# 人工智能

## 第二阶段

### （三）机器学习

- 概念

	- 从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。
	- 从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。

- 框架

	- scikit-learn库

		- 简介

			- 是基于 Python 语言的机器学习工具。

				- 官网：https://scikit-learn.org/stable/
				- 简单高效的数据挖掘和数据分析工具
				- 可供大家在各种环境中重复使用
				- 建立在 NumPy，SciPy 和 matplotlib 上
				- 开源，可商业使用 - BSD许可证

			- sklearn如何根据数据集选择模型？

		- 基础步骤

			- 1.加载数据
			- 2.划分训练集和测试集

				- train_test_split()

			- 3.数据预处理

				- 1）标准化

					- StandardScaler()

				- 2）归一化

					- MinMaxScaler()

				- 其他

			- 4.交叉验证法(用于模型选型和模型超参调优)

				- cross_val_score()

					- 手动编写循环进行交叉验证，并自行根据验证结果选择最优的模型和超参

				- GridSearchCV()

					- 自动调参。即只需输入可调参数列表，它自动进行交叉验证，并自动根据交叉验证结果选择最优的模型和超参

			- 5.创建模型
			- 6.模型拟合

				- fit()

			- 7.预测

				- predict()

			- 8.评估

				- 分类评估指标

					- 精确率(Accuracy)

						- accuracy_score()

					- F1分数(PR曲线)

						- f1_score()

					- 分类报告

						- classification_report()

					- 混淆矩阵

						- confusion_matrix()

					- Precision(精确率/查准率)
					- Recall(查全率)
					- ROC和AUC
					- 其他

				- 回归评估指标

					- 平均绝对误差(MAE)

							- mean_absolute_error()

					- 均方误差(MSE)

							- mean_squared_error()

					- 可解释方差

							- explained_variance_score()

					- R2判定系数

							- r2_score()

					- 其他

		- 回归

			- 普通线性回归

				- linear_model.LinearRegression()

			- 岭回归

				- linear_model.Ridge()

						- L2正则化

							- L2正则化可通过假设权重w的先验分布为高斯分布，由最大后验概率估计导出。
							- L2鼓励产生小而分散的权重，鼓励让模型做决策时考虑更多的特征，而不是仅仅强依赖于某几个特征，从而增强了模型的泛化能力，降低过拟合的风险。

					- 岭系数alpha对回归系数coef_的影响

			- LASSO回归

				- linear_model.Lasso()

						- L1正则化

							- L1正则化可通过假设权重w的先验分布为拉普拉斯分布，由最大后验概率估计导出。
							- L1鼓励产生稀疏的权重，即使得一部分权重为0，达到降维的效果。

			- 弹性网络

				- linear_model.ElasticNet()

						- L1和L2正则化项的综合

							- 当Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。

			- 贝叶斯岭回归

				- linear_model.BayesianRidge()

			- 核岭回归

				- kernel_ridge.KernelRidge()

					- 在岭回归的基础上添加核函数，增加了非线性的能力。
					- 核类型

						- linear(线性核)
						- polynomial(多项式核)
						- rbf(径向基核函数)
						- sigmoid
						- laplacian(拉普拉斯核函数)
						- 其他

			- SVM-->SVR

				- svm.SVR()

		- 分类

			- 逻辑斯蒂回归

				- linear_model.LogisticRegression()

						- 使用Sigmoid函数，将线性回归的结果映射到(0,1]内的一个值，h(x)=P(y=1|x; θ)表示在已知x和θ的条件下y=1的概率。
						- 注意：逻辑斯蒂回归的激活函数一定是Sigmoid，这个函数是推导出来的。

			- K最近邻算法(KNN)

				- neighbors.KNeighborsClassifier(n_neighbors=12, algorithm='kd_tree')

					- 原理

						- 所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分类到这个类中。

						- KNN的三个基本要素

							- 1）K值的选择

								- 首先，确定一个K值，表示离自己最近的样本数。

									- K值的选择会极大影响KNN的分类结果。
									- 若K值较小，预测结果对近邻的实例点非常敏感，如果近邻实例点恰好是噪声，那么预测就会出错。k值的减小意味着模型整体变复杂，容易发生过拟合。
									- K值较大，将使得模型趋于简单，容易发生欠拟合。

							- 2）距离度量

								- 然后，选择一种距离度量方式，计算待分类点与所有实例点的距离，并选择最近的K个点。

									- 一般采用Lp距离

										- p=2，对应L2范数，即欧氏距离。
										- p=1，对应L1范数，即曼哈顿距离。
										- 当p→∞时，对应无穷范数，即切比雪夫距离。

							- 3）分类决策规则

								- 最后，根据分类决策规则(往往是多数表决)来判定新样本所属类别。

					- KNN的两个重大缺陷

						- 1.当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数，导致分类错误。比如：一共有100个样本，分为A，B，C三类，但样本不平衡，A类有80个，B、C类各为10个。现在有一个待测样本X(实际是一个B样本)，KNN得到最近7个样本=4个A样本+3个B样本，于是根据多数表决，错误的将新样本归为A类。

							- 优化-->权值方法：和该样本距离小的邻居权值大。

						- 2.对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。当全体已知样本的容量较大时，算法耗时、搜索速度慢。如何快速的找到距离任意一个样本最近的K个样本点？

							- 优化-->KD-TREE：kd树是二叉树，是一种对k维空间实例点进行存储以便于快速检索的数据结构。

			- 梯度下降法

				- linear_model.SGDClassifier()

					- 分类

						- 随机梯度下降(SGD)

							- 每次迭代使用一个样本对参数进行更新。

						- 批量梯度下降(BGD)

							- 一次迭代使用所有样本对参数进行更新。

						- 小批量梯度下降(MBGD)

							- 前两者的折中方法，将总样本数分为多个batch，每次迭代使用batch_size个样本对参数进行更新。

			- SVM-->SVC

				- svm.SVC()

		- 支持向量机(Support Vector Machine，SVM)

			- SVC(Support Vector Classify)

				- svm.SVC()

					- 使到超平面最近的样本点的距离最大。
					- SVC用于解决二分类问题。

			- SVR(Support Vector Regression)

				- svm.SVR()

					- 使到超平面最远的样本点的距离最小。
					- SVR用于曲线拟合、函数回归，如预测温度、天气、股票等。

		- 聚类

			- K-Means算法

				- cluster.KMeans()

					- 步骤

						- 1.随机选取k个点，作为聚类中心。
						- 2.针对每个样本点，分别计算其到k个聚类中心的距离，然后将该点分到最近的聚类中心，这样就行成了k个簇。
						- 3.重新计算每个簇的质心（均值）。
						- 4.重复2、3步，直到质心收敛(质心位置不发生明显变化)或达到指定的迭代次数。

					- 优点

						- 原理简单(靠近中心点)，实现简单，聚类效果还不错。

					- 缺点

						- 1.无法确定K值(根据什么指标确定K值？)
						- 2.对离群点敏感(容易导致中心点偏移)
						- 3.算法复杂度不易控制，迭代次数可能较大。
						- 4.局部最优解而不是全局最优(这个和初始点选取有关，初始值不同一般得到不同的局部最优解。)

		- 数据预处理

			- z-score标准化(标准差标准化)

				- preprocessing.StandardScaler()

					- 去均值的中心化(均值变为0)，方差的规模化(方差变为1)。由X~N(μ,σ)，得到(X-μ)/σ~N(0,1)。
					- 适用于数据的分布本身就服从正态分布。

			- 将属性缩放到指定范围

				- min-max归一化

					- preprocessing.MinMaxScaler()

						- 结果在[0, 1]区间

				- MaxAbs归一化

					- preprocessing.MaxAbsScaler()

						- 每个特征除以绝对值的最大值
						- 结果在[-1, 1]区间
						- 不破坏原始数据的分布结构，适用于稀疏数据。

				- 四分位数归一化

					- preprocessing.RobustScaler()

							- Median=Q2是中位数，IQR=Q3-Q1是四分位距。

						- Median和IQR都采用加权的计算方式。即若总数N=2k是偶数，那么Median=0.5*Xk + 0.5*Xk+1。
						- 如果数据有许多异常值(异常值一般都位于两侧)，那么使用RobustScaler()可得到比较好的效果。

			- 将样本缩放到指定范围

				- L1归一化

					- preprocessing.Normalizer(norm="l1")

						- 每个特征除以L1范数(绝对值之和)

				- L2归一化

					- preprocessing.Normalizer(norm="l2")

						- 每个特征除以L2范数(欧氏距离)

				- 最大值归一化

					- preprocessing.Normalizer(norm="max")

						- 每个特征除以最大值

			- 二值化

				- preprocessing.Binarizer(threshold=)

					- 将数据转化为0和1，<=threshold的值转变为0, >threshold的值转变为1。可用于稀疏数据。

			- OneHotEncoder独热编码

				- preprocessing.OneHotEncoder()

					- # 独热编码
# 1.创建OneHotEncoder对象
enc = preprocessing.OneHotEncoder()

# 2.训练
# 4条样本数据，3个特征 进行训练
# 注意下面两者的区别
enc.fit([[0, 0, 3], [1, 2, 1], [0, 1, 2], [0, 0, 0]])
# enc.fit([[0, 0, 3], [1, 2, 1], [0, 1, 3], [0, 0, 0]])

# 3.使用
# 1条数据
ans = enc.transform([[1, 2, 3]]).toarray()
print(ans)
# 2条数据
ans = enc.transform([[0, 1, 3], [1, 0, 1]]).toarray()
print(ans)

			- 弥补缺失数据

				- sklearn.impute.SimpleImputer()

					- strategy="mean"

						- 平均数 填补

					- strategy="median"

						- 中位数 填补

					- strategy="most_frequent"

						- 众数 填补

		- 决策树(Decision Tree)

			- 概念

				- 在机器学习中，决策树是一个预测模型，代表对象属性和对象值之间的一种映射关系。
				- 决策树是一种树形结构，可用于分类和回归。其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。

			- 决策树的学习3步骤

				- 1）特征选择(即根据划分标准来选择最优划分特征)

					- 有3种划分标准

						- 1）信息增益

							- 表示得知X而使Y的不确定性减少程度。
						- 2）信息增益率

						- 3）基尼指数

								- 样本有K个类，样本点属于第k类的概率为pk。

							- G越大，数据不确定性越高，熵越大。

								- p1=p2...=pk=1/K时，G最大。

							- G越小，数据不确定性越低，熵越小。

								- G=0时，p1...pk中有一个值为1，其他值都为0，所有样本点都是同一个类别。

				- 2）决策树生成

					- 有3种生成算法

						- ID3-->根据信息增益进行划分
						- C4.5-->ID3的改进版，根据信息增益率进行划分
						- CART-->分类与回归树(Classification And Regression Tree)，根据基尼指数进行划分

				- 3）决策树剪枝

					- 为了简化决策树模型，降低过拟合的风险。

			- 决策树的优缺点

				- 优点

					- 决策树易于理解和实现，可以可视化分析，容易提取出规则。
					- 对于决策树，数据的准备往往是简单或者是不必要的，而且能够同时处理数据型和常规型属性，在相对短的时间内能够对大型数据源做出可行且效果良好的结果。
					- 易于通过静态测试来对模型进行评测，可以测定模型可信度。

				- 缺点

					- 1)对连续性的字段比较难预测。
					- 2)对有时间顺序的数据，需要很多预处理的工作。
					- 3)当类别太多时，错误可能就会增加的比较快。
					- 4)一般的算法分类的时候，只是根据一个字段来分类。

			- sklearn中的分类树模型

				- sklearn.tree.DecisionTreeClassifier()

			- sklearn中的回归树模型

				- sklearn.tree.DecisionTreeRegressor()

