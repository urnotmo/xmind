# 卷积神经网络

## 发展历史

### LeNet(1998，5层)

- LeNet-5是经典的手写数字识别模型

	- 首次提出卷积的概念，LeCun被称为CNN之父，LeNet被称为卷积神经网络的鼻祖。

### AlexNet(2012，8层)

- ILSVRC2012 第一名

	- 优化策略

		- 1）使用ReLU代替Sigmoid激活函数
		- 2）Dropout防止过拟合(随机使部分神经元失活)
		- 3）数据扩充增强数据集
		- 4）多GPU训练
		- 5）LRN局部归一化

			- Local Response Normalization，对本地神经元创建一种竞争机制，使其中响应比较大的值变得更大，并抑制其他反馈较小的神经元，从而增强模型的泛化性能。

		- 6）重叠池化

### VGG(2014，19层)

- ILSVRC2014 第二名

	- 可理解为加深版的AlexNet

### GoogLeNet(2014，22层)

- ILSVRC2014 第一名

	- 尽管增加模型的深度和宽度是提升性能的直接方式，但是深层网络面临几个问题：

		- 1）参数多，若训练数据集有限，易过拟合；
		- 2）网络越大计算复杂度越大，难以应用；
		- 3）网络越深，梯度越容易消失(梯度弥散)，模型训练难以收敛。

	- 核心思想：在增加模型的深度和宽度的同时减少参数量，即将全连接/一般的卷积-->稀疏连接。
	- 优化策略(创新点)

		- 深度：在中间增加两个辅助的softmax(整个网络的loss是三个loss的加权值)来避免梯度消失，从而可以设计更深的模型。
		- 宽度：设计了一种高效表达特征的稀疏性结构——Inception模块。

			- Inception v1网络结构

			- 1.使用1*1卷积进行升降维；2.在多个尺寸上同时进行卷积再聚合(concat)。

### ResNet(2015，152层)

- ILSVRC2015图像识别、图像检测、图像定位等多项冠军。

	- 背景

		- 在使用标准初始化和中间层正规化(如Batch Normalization)有效控制梯度弥散后，即使得模型训练可以收敛的前提下，引出了另一个问题——网络退化问题。
		- 网络退化问题：在神经网络可以收敛的前提下，随着网络深度增加，网络的表现先是逐渐增加至饱和，然后迅速下降。

			- 试验结果如下图所示，这并非是一个过拟合问题，56-layer的训练集和测试集误差都比20-layer高。
		- 试验的结果并不符合常理：若某个K层的网络f是当前最优的网络，那么总可以构造一个更深的网络，其最后基层仅是该网络f第K层输出的恒等映射(Identity Mapping)，就可以取得一致的结果；也许K还不是所谓"最佳层数"，那么更深的网络就可以取得更好的效果。也就是说，与浅层网络相比，更深的网络的表现不应该更差。因此，得到一个合理的猜测，对于神经网络来说，恒等映射函数并不容易拟合。

	- 核心思想

		- 既然神经网络不容易拟合一个恒等映射，那么我们就人为的构造天然的恒等映射。
		- 将恒等映射函数H(x)=x变形为残差函数F(x)=H(x)-x，只要F(x)=0就等价于构成了一个恒等映射H(x)=x。而且，拟合残差F(x)=0或F(x)-->0更加容易。
		- 残差块结构

				- 结构优化

			- F(x)是残差输出，x是用shortcut/跳跃连接实现的一个恒等映射，直接将F(x)和x相加在一起，再做ReLU激活。

### DenseNet(2017，)

- CVPR 2017最佳论文

	- 设计理念：相比ResNet，提出了一个更激进的密集连接方式——通过建立前面所有层与后面层的密集连接(dense connection)，并通过channel上的concat实现特征重用(feature reuse)。
	- 结构

