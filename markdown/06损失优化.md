# 人工智能

## 第三阶段(深度学习进阶)

### 损失/训练优化

- 人脸识别损失设计

	- 早期网络

		- 1.孪生网络

			- 缺陷：B与A(原始图片)进行比较时，同时将B与A输入网络中再做对比，而没有将A图片的原始特征保存下来。因此每次比较都要将原始图片放到模型中，速度慢、无法实时、不可商业化。

	- 基于Euclid Distance的损失函数

		- 2.Triplet loss(三元组损失)

				- triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本和不同类的样本,这两个样本对应的称为Positive (记为x_p)和Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。
				- 这里距离用欧式距离度量，+表示[]内的值大于零的时候，取该值为损失，小于零的时候，损失为零。
				- "+样本"如右图

			- 缺陷：随着模型的收敛，当Triplet loss下降到一定的程度时，"+样本"数量越来越少，只能通过不断加入新的样本来增加"+样本"数量。因此，Triplet Loss收敛困难，并且需要大量的数据，耗时长。

		- 3.CenterLoss(中心损失)

			- 减少类内距，间接增大类间距

	- 基于Angular Margin的损失函数

		- Softmax Loss及其优化损失

			- 0）Softmax Loss

					- softmax损失仅鼓励特征的可分离性，所得到的特征对于人脸识别不是足够有效的。

			- 1）L-Softmax Loss(Large-Margin Softmax Loss)

				- 公式

						- 条件偏置b=0

				- 原理

					- 增加一个正整数变量m，从而产生决策余量，能够更加严格的约束上述不等式，即：
					- 其中m是正整数，cos函数在0到π范围又是单调递减的，所以cos(mx)要小于cos(x)。m值越大则学习的难度也越大，使得模型可以学到类间距离更大的，类内距离更小的特征。
					- 以一个二分类问题为例，示意图：

			- 2）A-Softmax Loss

				- 在L-Softmax loss(b=0)的基础上添加了条件||W||=1。
			- 3）CosFace Loss(与AM-Softmax Loss一样，不同人提出的同一种优化损失)

			- 4）ArcFace Loss

- 交叉熵

	- 二分类

	- 多分类

