# 人工智能

## 第三阶段(深度学习进阶)

### 强化学习(了解即可)

- 基本原理

- 马尔可夫链

- 贝尔曼方程

- Qlearning

	- 根据Q函数迭代计算Q值

		- Q函数

		- 最终

- DQN

### PyTorch模型部署(重点掌握)

- 知识点

	- 部署方式(2种)

		- HTTP协议

			- 如：Flask部署

		- Socket通讯协议

			- 优点：快

	- 转换格式(2种)

		- PyTorch模型-->TorchScript序列化

			- 可使用C++等高性能环境加载TorchScript模型。

		- PyTorch模型-->ONNX模型

			- 1）直接用ONNX Runtime运行ONNX模型。
			- 2）安装TensorRT，并使用TensorRT优化推理ONNX模型。

				- TensorRT是专门针对推理而设计的，推理能力是cuda的几倍。

	- 部署分类

		- PC端
		- 服务端
		- 移动端

			- iOS
			- Android

		- IOT(物联网)设备

			- GPU

				- NVIDIA Jetson：适用于一切自主机器的嵌入式系统

					- Jetson Nano：128 个 NVIDIA CUDA核心
					- Jetson TX2：具有 256 个 NVIDIA CUDA 核心
					- Jetson Xavier NX：配备 384 个 NVIDIA CUDA 核心和 48 个 Tensor 核心
					- Jetson AGX Xavier：具有 512 个 NVIDIA CUDA 核心和 64 个 Tensor 核心

				- 华为海思
				- 瑞芯微

			- CPU

				- 树莓派

- 部署PyTorch模型

	- 一、PyTorch模型格式转换

		- 1.PyTorch模型-->TorchScript序列化

			- 1)了解TorchScript

				- https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html

			- 2)用C++加载一个TorchScript模型

				- https://pytorch.org/tutorials/advanced/cpp_export.html

			- 缺点：推理时依然依赖PyTorch框架，是PyTorch内置的部署方式。

		- 2.PyTorch模型-->ONNX模型, 并用ONNX Runtime运行

			- https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html

			- 步骤

				- 1）PyTorch模型导出为ONNX
				- 2）加载ONNX模型，并用ONNX的API检查ONNX模型
				- 3）在ONNX Runtime上运行模型

			- 优点：跨多个平台和硬件，不依赖原始深度学习框架。

	- 二、部署方式

		- 1.Flask部署PyTorch模型

			- https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html

			- 缺点

				- 1）最简单的部署PyTorch模型的方式，但不适用于需要高性能需求的用例。

		- 2.Socket部署PyTorch模型

			- 1）学习网路编程

				- https://www.runoob.com/python3/python3-socket.html

	- 三、TensorRT优化推理引擎

		- 官网

			- https://developer.nvidia.com/zh-cn/tensorrt

			- NVIDIA的TensorRT是针对前向推理设计的，推理速度是cuda的好几倍。

		- 1.安装TensorRT

			- https://blog.csdn.net/zong596568821xp/article/details/86077553

			- 步驟

				- 1）卸载cuda，重装TensorRT支持的cuda版本。

					- TensorRT对应不同的cuda版本，有可能不匹配。
					- https://www.jianshu.com/p/594b34ea0347

						- 已完成：cuda10.2，cudnn7.6.5。

				- 2）下载TensorRT安装包解压，并配置TensorRT环境变量。

					- 将\TensorRT-7.0.0.11\lib目录下的.dll文件拷贝到\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin目录下
					- 环境变量Path中添加D:\AI\TensorRT-7.0.0.11\lib

		- 2.使用TensorRT

			- 步骤

				- 1）双击sln文件直接用Visual Studio2017打开项目代码。

				- 2）项目-->属性-->目标平台改为Windows。

				- 3）生成-->生成解决方案，编译代码。
				- 4）调试-->开始执行(或Ctrl+F5或直接点击执行按钮)，执行代码。

			- sampleOnnxMNIST.cpp项目代码大致流程

				- 1）初始化参数

					- 图片数据、onnx模型文件、模型输入input、输出output等。

				- 2）构建

					- 创建网络：通过解析Onnx模型来创建Onnx MNIST网络。
					- 创建网络引擎：构建将用于运行MNIST的引擎。

				- 3）推理

					- 运行TensorRT推理引擎进行推理。

						- 1）读取输入，执行推理，并将结果存储在缓存区中。
						- 2）读取缓存区中的结果进行验证。

	- 四、PyTorch Mobile框架

		- 官网

			- https://pytorch.org/mobile/home/

		- 简介

			- Fackbook发布的PyTorch Mobile框架，专门用于支持移动端的部署。

				- iOS
				- Android

			- 该功能目前处在早期实验阶段，特性包括：

				- 1）提供 API，涵盖将 ML 集成到移动应用中所需的常见预处理和集成任务。
				- 2）支持 QNNPACK 量化内核库和 ARM CPU 支持。
				- 3）根据用户的应用需求进行构建级别的优化和选择性编译。
				- 4）进一步改善移动 CPU 和 GPU 的性能和覆盖范围。

			- 从训练到部署到移动端的工作流程/步骤：

### AlphaZero(了解)

- 代码，百度网盘

	- 链接：https://pan.baidu.com/s/1yzsxVmUibQR7ELmoY_KKbg
	- 提取码：s64b

- 1.MCTS(蒙特卡洛树)
- 2.神经网络

### 模型压缩

- 深度学习因其计算复杂度或参数冗余，在一些场景和设备上限制了相应模型部署，需要借助模型压缩、优化加速、异构计算等方法突破瓶颈。
- 模型压缩算法能够有效降低参数冗余，从而减少存储占用、通信带宽和计算复杂度，有助于深度学习的应用部署，可分为：

	- 1）线性或非线性量化：1/2bits，int8和fp16等；
	- 2）结构或非结构剪枝；
	- 3）知识蒸馏与网络结构简化等。

- 剪枝

	- 方式

		- 非结构化剪枝：通常是连接级、细粒度的剪枝方法，精度相对较高。

		- 结构剪枝：是filter级或layer级、粗粒度的剪枝方法，精度相对较低，但剪枝策略更为有效。

	- 剪枝一般都是逐级而非一次性：

- 量化

	- 常规精度：FP32，低精度：FP16，INT8。
	- 量化一般指INT8。

	- 量化操作

- 知识蒸馏

	- 通过使用一个较大的已经训练好的网络去教导一个较小的网络。

