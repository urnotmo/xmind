# Attention、Focal Loss、mAP

## Attention注意力机制

### 1.Attention相关概念

- 来源

	- 最早用于NLP(自然语言处理)，后来在CV中也得到广泛的应用。

- 作用

	- 1）在神经网络中，注意力机制通常是一个额外的神经网络，能够硬性地选择输入的某些部分，或者给输入的不同部分分配不同的权重。
	- 2）注意力机制能够从大量信息中筛选出重要的信息。

- 如何引入attention机制

	- 以CNN为例，可以在空间维度上增加attention机制，可以在channel维度上增加attention机制，还可以同时在空间维度和channel维度上引入attention机制(不过计算量比较大)。
	- 1）在channel-wise上增加注意力机制

		- 以Squeeze-and-Excitation Networks为例

			- 三个操作实现：

				- 1.Squeeze(挤压)

					- 通过全局池化(global pooling，论文是平均池化)，将每个通道的二维特征(h*w)压缩为1个实数。
					- 通过squeeze操作后变为1*1*C，实数计算公式：

				- 2.excitation(激励)

					- 用一个瓶颈结构(两个全连接层组成的Bottleneck)，为每个channel生成一个权重值。
					- SE block如何嵌入到主流网络(inception、resnet)中？

						- 1）SE-Inception-Module

						- 2）SE-ResNet-Module

					- 具体过程：

						- 1）首先，通过一个FC将特征维度降低到原来的1/r(即C/r)；
						- 2）然后，经过ReLu激活函数激活；
						- 3）再通过一个FC将特征维度还原成C；
						- 4）最后，通过sigmoid函数转化为一个0~1的归一化权重。

				- 3.Scale(缩放)

					- 将得到的归一化权重加权(论文是直接相乘)到每个通道的特征上，完成在通道维度上引入attention机制。

			- 补充

				- 1）SE-net的核心思想：通过全连接网络，根据loss去自动学习特征权重，使得有效的特征通道权重大。当然SE block不可避免地增加了一些参数和计算量，但是在效果面前，这个性价比还是很高的。
				- 2）原作者认为在excitation操作中使用两个全连接层比直接用一个全连接层更好，体现在：

					- 1.更多的非线性；
					- 2.更好的拟合通道间的复杂度。

### 2.公式

### 3.分类

- 单头注意力

	- 只有一个QKV(“同时”只能关注一个地方)

- 多头注意力
- 自注意力：QKV来自同一个输入。

## Focal Loss

### 1）原有交叉熵的基础上添加一个因子gamma=2(>0)，降低了易分类样本的权重，使得更关注于困难的、错分的样本。

### 2）引入平衡因子alpha=0.25，用来平衡正负样本比例不均。

## 稀疏编码

### 1）稀疏编码是一种无监督学习方法，它用来寻找一组"超完备"基向量来高效地表示样本数据。

### 2）稀疏编码算法的目的，就是找到一组基向量，使得我们能够将输入向量表示为这组向量的线性组合。

## 目标检测指标mAP

### 参考资料

- 视频

	- https://www.bilibili.com/video/BV1ez4y1X7g2?from=search&seid=2237284758916066224

- 博客

	- https://zhuanlan.zhihu.com/p/37910324

### 根据训练数据中各个类的分布情况，mAP值可能在某些类(具有良好的训练数据)非常高，而在其他类(具有较少/不良数据)却比较低。所以你的mAP可能是中等的，但是你的模型可能对某些类非常好，对某些类非常不好。因此，建议在分析模型结果时查看各个类的AP值，这些AP值也许暗示你需要添加更多的训练样本。

