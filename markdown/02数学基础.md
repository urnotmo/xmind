# 人工智能

## 第二阶段

### （一）数学基础

- 高等数学

	- 函数

		- 函数分类

			- 基本初等函数

				- 幂函数
				- 指数函数
				- 对数函数
				- 三角函数
				- 反三角函数
				- 常数函数

			- 初等函数（由基本初等函数有限次复合而成）

		- 激活函数

			- Sigmoid

			- tanh

			- ReLU

			- Leaky ReLU

			- ELU

		- 凸函数和凸集

			- 凸函数（任意两点连线位于曲线上方）
			- 凸集（任意两点连接位于集合内部）

		- 反函数(关于y=x对称)

	- 极限
	- 导数

		- 在一元函数f(x)中，导数就是函数的变化率

			- 一阶导数决定增减
			- 二阶导数决定凹凸
			- 三阶导数决定偏度
			- 四阶导数决定峰度

	- 偏导数

		- 反映函数沿坐标轴方向的变化率

	- 方向导数

		- 反映函数在其他特定方向(任意方向)上的变化率

	- 梯度

		- 梯度是一个向量，并且总是指向函数增长最快的方向
		- 梯度的方向是最大方向导数的方向
		- 梯度的值是最大方向导数的值

	- 微积分(以直代曲)

- 线性代数

	- 理解线性

		- 如何求解多元线性方程组？-->矩阵、矩阵乘法
		- 理解机器学习模型中的过拟合、拟合、欠拟合

			- 过拟合：数据量少、参数过多；相当于一个方程组有无数多个解。
			- 拟合：数据量与参数相匹配；方程组有且只有一个解。
			- 欠拟合：数据量大、参数过少。无法求得方程组的正解。

		- 线性可分与线性不可分

			- 将低维线性不可分问题转换为高维线性可分问题

	- 张量

		- 分类

			- 标量（Scalar，零阶张量r=0）
			- 向量（Vector，一阶张量r=1）
			- 矩阵（Matrix，二阶张量r=2）
			- 高阶张量（r>=3）

		- 基本运算

			- 加减法
			- 点乘
			- 数乘
			- 叉乘

	- 最小二乘法

			- 最优化算法：最小二乘法 vs 梯度下降法
1、最小二乘法：
    1）一次计算即可得到最优解(全局最优解)。
    2）只适用于线性模型，不适用于逻辑回归等其他问题。
    3）当特征数n较大时，计算矩阵逆的时间复杂度太大，费时。
2、梯度下降法
    1）需要选择学习率，需要多次迭代找到最优解(局部最优解)。
    2）适用于多种类型的模型。
总结:对于非凸函数，梯度下降法无法保证得到全局最优解。
    最小二乘法虽然可以得到全局最优解，但当特征数比较大时，计算太费时。
    最小二乘法只适用于线性模型，梯度下降法适用性极强。

	- 范数相关

		- 范数

			- 范数常被用来度量某个向量空间(或矩阵)中的向量的长度或大小。
			- 分类

				- 0范数：向量中非0元素的个数
				- 1范数：绝对值之和
				- 2范数：通常意义上的模，又称欧几里得范数。
				- 无穷范数：取向量中绝对值最大的那个向量的绝对值

		- 归一化(Normalize)：简化计算的一种方式。

			- 在机器学习中，归一化通常将数据限制在[-1, +1]之间。

				- 2范数归一化
				- 无穷范数(最大值)归一化
				- 均值(期望)归一化

	- 行列式

		- 只有方正有行列式，n阶矩阵A的行列式记作det(A)或 | A | 。行列式为0叫奇异矩阵，行列式不为0称非奇异矩阵。
		- 行列式可以看做是有向面积或体积的概念在一般的欧几里得空间中的推广。或者说，在n维欧几里得空间中，行列式描述的是一个线性变换(矩阵)对"体积"所造成的影响。
		- 行列式是线性变换(矩阵)的伸缩因子

			- 行列式>1：对图形有放大效果
			- 行列式=1：图形大小不变
			- 0<行列式<1：对图形有缩小的作用
			- 行列式=0：原本图形被压缩成一个点(或一条直线)。这也是为什么说行列式为0的线性变换(矩阵)是不可逆，当图形被压缩成一个点或一条直线时，没有线性变换(矩阵)可以把他们恢复成原来的样子。
			- 行列式<0：改变了基的左右准则，可理解为翻转。

	- 特殊矩阵

		- 方正：行列数相同，n x n
		- 0(1)矩阵：元素全为0(1)的矩阵
		- 单位矩阵：主对角线全为1，除此外全为0
		- 下(上)三角矩阵：主对角线上(下)方全为0
		- 对角矩阵：只有对角线上有非0元素的矩阵
		- 稀疏矩阵：0元素数量远大于非0元素，且非0元素分布没有规律。与之相反，非0元素数量远大于0元素，称稠密矩阵。

	- 线性变换

		- 在由一组基组成的线性空间内，对线性空间中的数据进行平移、旋转、翻转、缩放等操作叫做线性变换。矩阵就是一个线性变换，由连续的线性变换，引入了矩阵乘法的概念；矩阵乘法(叉乘)的结果是连续两次线性变化合并成的线性变换。

	- 相似矩阵相关

		- 向量的內积(即点乘)

			- 几何意义：
1、表征或计算两个向量之间的夹角；
2、a向量在b向量上的投影。

		- 向量组的线性相关性

			- 线性相关

			- 线性无关

		- 方正的特征值和特征向量：Ax=λx

			- 设A是n阶方阵，如果数λ和n维非零列向量x使关系式Ax=λx成立，那么这样的数λ称为矩阵A的特征值，非零向量x称为A的对应于特征值λ的特征向量。
			- 不同的特征值所对应的特征向量是线性无关的。
			- 特征值和特征向量主要用于主成分分析(如数据降维、图片压缩等)。

		- 相似矩阵

				- 相似矩阵的本质：同一线性变换，在不同基下的矩阵。
				- 相似矩阵之间具有相同的特征值。
				- 通过矩阵(线性变换)的相似对角化，可以解耦变量以便简化问题。

	- 奇异值分解相关

		- 谱范数相关

			- 谱半径：矩阵A的所有特征值绝对值集合的上确界，即最大的特征值绝对值。
			- 谱范数：A的最大奇异值(奇异值是非负的)。
			- 谱范数归一化：用谱范数做归一化。

		- 奇异值分解(SVD)

- 概率论

	- 基本概念

		- 样本空间、随机事件

			- 样本点：每一个可能出现的试验结果称为一个样本点。
			- 样本空间Ω：随机试验所有可能结果组成的集合，即全体样本点组成的集合。
			- 事件

				- 随机事件
				- 确定事件

					- 必然事件(Ω是必然事件)
					- 不可能事件(∅是不可能事件)

			- 事件的运算

		- 等可能概型

			- 古典概型

				- 有限性（所有可能出现的基本事件只有有限个）
				- 等可能性（每个基本事件出现的可能性相等）

		- 独立性：若A、B两个事件满足 P(AB) = P(A)P(B)，那么称事件A、B相互独立。
		- 贝叶斯公式

			- 条件概率、联合概率、边缘概率

				- 条件概率：事件B发生的情况下，事件A发生的概率，即P(A=a|B=b)，记作 P(A|B)。
				- 联合概率：在多元概率分布中，多个条件同时成立的概率，即P(X=a, Y=b)，记作P(AB)。
				- 边缘概率：在多元概率分布中，与联合概率对应的，单个随机变量的概率，即P(X=a)或P(Y=b)，记作P(A)或P(B)。

			- 全概率公式

			- 先验概率、似然函数、后验概率

				- 先验概率：一个事情发生前我们大概知道这件事情发生的概率。它一般是根据以往经验和分析得到的概率。
				- 似然函数：给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：L(θ|x)=P(X=x|θ)。
				- 后验概率：当某个与之关联的事情发生后，我们去重新定义这个事情发生的概率。后验概率的计算要以先验概率为基础，在贝叶斯公式中，用先验概率和似然函数计算出来。

			- 贝叶斯公式

			- 朴素贝叶斯

				- 以贝叶斯定理为基础，并且假设特征条件之间相互独立。
				- 优点：①由于假设了数据集属性之间是相互独立的，因此算法的逻辑性十分简单。②朴素贝叶斯对于不同类型的数据集不会呈现太大的差异性，健壮性比较好。③当数据集属性之间的关系相对比较独立时，朴素贝叶斯算法有较好的分类效果。
缺点：假设属性独立性的条件也是朴素贝叶斯的不足之处。数据集之间往往都存在着相互关联，如果数据集间关联紧密，那么分类效果就会大大降低。

	- 概率分布

		- 概念

			- 是指用于表述随机变量取值的概率规律

		- 特点

			- 每个随机变量的概率值范围[0, 1]
			- 所有概率值总和为1

		- 分类

			- 离散概率分布：随机变量为离散数据

				- 常见类别

					- 伯努利分布/0-1分布/两点分布

						- 列表表示

						- 公式

					- 二项分布

						- 特点

							- 1）做某件事的次数(试验次数)是固定的，用n表示。
							- 2）每次事件都有两个可能的结果(成功或失败)。
							- 3）每次成功的概率是相等的，成功的概率用p表示。

						- 公式

						- 一般地，如果随机变量X服从参数为n和p的二项分布，我们记为X~B(n, p)或X~b(n, p) 。

					- 泊松分布

			- 连续概率分布：随机变量为连续数据

				- 性质

					- 对于连续型随机变量，用概率密度函数(简称概率密度或密度函数)来描述一个随机变量的输出值，在某个确定取值点附近的可能性。
					- 连续随机变量取值在某个区间内的概率，为概率密度函数在该区间上的定积分，即概率密度曲线下对应区间的面积。概率密度曲线下的总面积为1。
					- 连续型随机变量取值在任意一点的概率都是0。

				- 常见类别

					- 均匀分布/矩形分布

						- 在区间(a,b)上取值是等可能性的。概率密度函数 f(x)=1/(b-a), a<x<b。

					- 高斯分布/正态分布/常态分布(Normal distribution)

						- 特征缩放(Feature Scaling)

							- 特征伸缩(Feature Scaling)是数据预处理的一个关键步骤，可以提高模型精度，提高收敛速度。
							- 在机器学习中，Feature Scaling的常见方法有：归一化和标准化。

								- 归一化：将数据映射到[0, 1]

									- 适用于线性模型，非线性模型就不用归一化。
									- 归一化可消除由奇异样本数据引起的训练时间过长、无法收敛等问题。

								- 标准化：把数据变为均值为0、方差为1的正太分布

									- 适用于数据本身服从正太分布。
									- 标准化可消除分布产生的度量偏差。

	- 随机变量的数字特征

		- 数学期望(简称期望，又称均值)

			- 衡量数据的中心点(平均值)
			- 性质

				- 1）C是常数，有 E(C)=C
				- 2）C是常数，有 E(CX)=CE(X)
				- 3）X和Y是随机变量，有 E(X+Y)=E(X)+E(Y)
				- 4）X和Y是相互独立的随机变量，有 E(XY)=E(X)E(Y)

		- 方差

			- 衡量数据的离散程度(稳定性)：方差越大，波动越大，越离散，越不稳定。
		- 标准差/均方差

			- 是方差的算术平方根

		- 协方差

			- 衡量两个变量的总体误差。方差是协方差的一种特殊情况，即两个变量相同的情况。
			- 性质

				- 1°Cov(X,Y)=0，称X和Y不相关。注意：若X和Y是相互独立的，那么Cov(X,Y)=0；但是反过来，若Cov(X,Y)=0，不一定是独立的。
				- 2°Cov(X,Y)>0，称X和Y正相关。两个变量的变化趋势一致。
				- 3°Cov(X,Y)<0，称X和Y负相关。两个变量的变化趋势相反。

	- 大数定理及中心极限定理

		- 大数定理：在试验条件不变的条件下，重复试验多次，随机事件的频率近似于它的概率。重复次数越多，概率越准确。
		- 中心极限定理：在自然界与生产中，一些现象受到许多相互独立的随机因素的影响，如果每个因素所产生的的影响都很微小时，总的影响可以看作是服从正态分布的。

	- 参数估计

		- 极大似然估计

			- 思想：已知某个参数能使这个样本出现的概率最大，干脆就把这个参数作为估计的真实值。

- 信息论

	- 信息与熵

		- 自信息

		- 信息熵

		- 联合熵

		- 条件熵

		- 交叉熵

		- 相对熵/信息散度/KL散度

		- 互信息

