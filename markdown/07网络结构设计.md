# 人工智能

## 第三阶段(深度学习进阶)

### 网络结构设计

- 1）Batch Normalization

	- 论文

		- Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
		- 下载地址：https://arxiv.org/abs/1502.03167

	- 前提

		- Internal Covariate Shift(内部协变量偏移)：输出由于受到参数和激活函数等的影响，当参数进行更新变化时，输出的分布发生变换/偏移(与输入的分布不一致)，特别地，对于深层神经网络，这种变化会被放大(类似蝴蝶效应)，导致模型不易训练/收敛。
		- Internal Covariate Shift带来的问题：上/后层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低。

	- 思想

		- 1）单独对每个特征进行normalizaiton，让每个特征都有均值为0，方差为1的分布，可以减缓ICS的问题。
		- 2）既然白化操作"可能"减弱了网络中每一层输入数据的表达能力，我们就再加一个线性变换操作，让数据尽可能地恢复本身的表达能力。特别地，若用tanh或sigmoid激活函数时，标准正太分布容易陷入非线性激活函数的线性区域。

			- 如右图，标准正太分布数据68%在[-1, 1]之间，而该区间sigmoid和tanh的非线性能力弱。

	- 计算公式

		- 训练阶段

				- 先做标准归一化，再做伸缩scale和平移shift变换。γ，β是两个可训练的参数，ε是为了防止方差为0产生无效计算。
				- 利用BN训练好模型后，我们保留了每组mini-batch训练数据在网络中每一层的均值和方差。

		- 预测阶段

				- 使用全部训练样本的统计量(即均值和方差的无偏估计)，来作为测试数据的均值和方差。
				- 关于无偏估计，参考链接：https://blog.csdn.net/qq_16587307/article/details/81328773

				- 先做标准归一化，再做再做伸缩scale和平移shift变换。γ，β是已经训练好的参数，ε是为了防止方差为0产生无效计算。

	- 补充

		- 在进行Normalization的过程中，由于我们的规范化操作会减去均值，有BN(Wu+b)=BN(Wu)，因此若BN加在激活函数之前时，偏置项b可以被忽略掉或被置为0。

	- 优势

		- 1）BN使得网络中每层输入数据的分布(均值和方差)相对稳定，减少了网络中Internal Covariate Shift问题，并在一定程度上缓解了梯度消失，加速模型学习速度(模型更易训练、收敛)。
		- 2）BN使得模型对网络中的参数不那么敏感，简化调参过程，使网路学习更加稳定。

			- 1）会对数据进行规范化操作，因此对权重初始化的要求就不用那么严格。
			- 2）经过BN操作以后，权重的缩放值会被“抹去”。

		- 3）BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），线性变换的γ，β由网络自学得到。
		- 4）BN具有一定的正则化(防止过拟合)效果。

			- 使用mini-batch的均值和方差作为整体训练样本均值和方差的统计量估计，尽管每一个batch的数据都是从总体样本中抽样得到，但不同的mini-batch的均值和方差会有不同，这就为网络学习增加了随机噪声。与Dropout通过随机关闭神经元来给网络训练添加随机噪声类似，在一定程度上对模型起到了正则化的效果。

- 几种归一化类型

	- BN，Batch Normalization

		- 对batch_size个样本的每个特征进行normalizaiton，即在N、H、W上操作而保留通道C的维度。
		- 均值和标准差的计算公式：

		- 缺点

			- 需要较大的batch_size才能合理估计训练数据的均值和方差，这导致内存很可能不够用。同时它很难应用在训练数据长度不同的RNN模型上(RNN的训练数据长度可以不一致，这个时候就能将多个数据放到同一个batch中)。

	- LN，Layer Normalization

		- 对单个样本的所有特征(按层)进行normalizaiton，即在C、H、W维度上操作，保留N维度。
		- 均值和标准差的计算公式：

		- 独立于batch，对单个样本的所有特征做归一化，常用于RNN网络中。若输入的特征区别很大，则不建议用LN做归一化。

	- IN，Instance Normalization

		- 对单个样本的每个特征进行normalizaiton，即在H、W上操作而保留通道N、C的维度。
		- 均值和标准差的计算公式：

		- 独立于batch，常用来风格化迁移，但如果输入的特征之间相关性较强，则不建议用IN做归一化。

	- GN，Group Normalization

		- GN也是独立于batch的，它是LN和IN的折中，对单个样本的C/G个特征进行normalizaiton。
		- 均值和标准差的计算公式：

	- WN，Weight Normalization

		- BN的变种，区别于BN对数据进行normalization，它是对网络权值W进行normalization。

- 2）Dropout

	- 分类

		- 传统的Dropout

			- 在训练阶段，每个神经元以p概率丢弃(使用部分神经元)。
			- 在预测阶段，使用所有神经元，将得到的值乘以scale=(1-p)作为output。

		- 反向Dropout(Inverted Dropout)

			- 在训练阶段，每个神经元以p概率丢弃(使用部分神经元)，将得到的值乘以scale=1/(1-p)作为output。

			- 在预测阶段，使用所有神经元，将得到的值直接作为output。

	- 补充

		- 预测时乘以(1-p)是传统的Dropout；训练时乘以1/(1-p)是Inverted Dropout。Inverted Dropout是现在主流的Dropout技术，Keras、PyTorch等框架实现的就是Inverted Dropout。(Keras是一个由Python编写的开源人工神经网络库)
		- 如何随机使部分神经元失活？

	- 如何防止过拟合？

		- 1）(传统Dropout)随机失活部分神经元，相当于减小模型。
		- 2）(传统Dropout和Inverted Dropout)通过随机关闭神经元来给网络训练添加随机噪声，在一定程度上对模型起到了正则化的效果。

- PyTorch的API

	- model.train()和model.eval()

		- model.train()  # Set model to training mode
		- model.eval()   # Set model to evaluate mode

			- 一般与with torch.no_grad():或@torch.no_grad()结合使用，表示不需要计算梯度/反向传播，可以节省显存空间。

		- 如果模型中有BN层(Batch Normalization)或Dropout，需要在训练时添加model.train(True)，在验证/评估时调用model.train(False)或model.eval()。model.eval() 等价于 model.train(mode=False)。

			- 对于BN，model.train()是保证BN层用每一batch数据的均值和方差，
而model.eval()是保证BN用全部"训练"数据的均值和方差(global mean/var)；
			- 对于Dropout，model.train()是随机取一部分网络连接来训练并乘以1/(1-p)，
而model.eval()是利用到了所有网络连接。

	- net.apply(init_weights_fun)

		- nn.Module类的方法，遍历net模型中的所有子module，调用init_weights_fun(module)初始化参数。

		- model.children()获取model中的所有子Module的迭代器对象

- 问题

	- 传统Dropout和Inverted Dropout哪个好？

	  ？

		- 关于两者哪个效果更好，查看了一些资料都没有明确说明，有一种说法是Inverted Dropout将乘以伸缩系数的操作移动到训练上，预测/使用时更方便。

	- BN与Dropout的冲突及解决策略？

	  ？

		- https://www.zhihu.com/question/265819518?sort=created

			- 由于Dropout训练时用随机部分神经元，而使用时使用所有神经元，会发生"variance shift"(方差偏移)，是导致两者冲突的原因。

		- https://zhuanlan.zhihu.com/p/61725100

			- 解决策略

				- 方案1）将Dropout放在所有BN层的后面。
				- 方案2）使用均匀分布Dropout(简称"Uout")。Uout可使整体方差偏移变小。

	- BN与激活函数的顺序？

	  ？

		- 1）一般情况下，是先激活再使用BN，因为BN的作用就是保持激活值的均值和方差不变。
		- 2）但是对于ReLU激活函数，先BN再ReLU，如ResNet、DenseNet都是conv+bn+relu结构。因为如果先ReLU，那么BN的时候很多神经元失活导致BN不稳定，影响模型性能。

- 防止overfitting的几种方法

	- 1）L1/L2正则化直接对训练参数添加某种惩罚来防止过拟合。
	- 2）数据增强
	- 3）训练提前停止(early stopping)
	- 4）Dropout

		- 传统Dropout

			- 随机使部分神经元失活，相当于减小模型。
			- 通过随机关闭部分神经元，给网络训练添加随机噪声。

		- Inverted Dropout

			- 通过随机关闭部分神经元，给网络训练添加随机噪声。

	- 5）BN：通过使用mini-batch的均值和方差作为整体训练样本均值和方差的统计量估计，给网络训练添加随机噪声。

- 3）权重初始化

	- 对权重的大小和正负缺乏先验，一般初始化在0附近，有一定的随机性，数学期望E=0。注意：权重初始化不能为全0或常数。
	- 根据中心极限定理，一般将权重初始化为标准正太分布。并非绝对，也可使用均匀分布来初始化，目前效果最好的是截断正太分布。
	- 权重的初始化要考虑"前向和后向"两个过程，防止梯度消失和梯度爆炸。
	- 发展史

- 4）卷积神经网络的发展史

	- LeNet(1998，5层)

		- LeNet-5是经典的手写数字识别模型

			- 首次提出卷积的概念，LeCun被称为CNN之父，LeNet被称为卷积神经网络的鼻祖。

	- AlexNet(2012，8层)

		- ILSVRC2012 第一名

			- 优化策略

				- 1）使用ReLU代替Sigmoid激活函数
				- 2）Dropout防止过拟合(随机使部分神经元失活)
				- 3）数据扩充增强数据集
				- 4）多GPU训练
				- 5）LRN局部归一化

					- Local Response Normalization，对本地神经元创建一种竞争机制，使其中响应比较大的值变得更大，并抑制其他反馈较小的神经元，从而增强模型的泛化性能。

				- 6）重叠池化

	- VGG(2014，19层)

		- ILSVRC2014 第二名

			- 可理解为加深版的AlexNet

	- GoogLeNet(2014，22层)

		- ILSVRC2014 第一名

			- 尽管增加模型的深度和宽度是提升性能的直接方式，但是深层网络面临几个问题：

				- 1）参数多，若训练数据集有限，易过拟合；
				- 2）网络越大计算复杂度越大，难以应用；
				- 3）网络越深，梯度越容易消失(梯度弥散)，模型训练难以收敛。

			- 核心思想：在增加模型的深度和宽度的同时减少参数量，即将全连接/一般的卷积-->稀疏连接。
			- 优化策略(创新点)

				- 深度：在中间增加两个辅助的softmax(整个网络的loss是三个loss的加权值)来避免梯度消失，从而可以设计更深的模型。
				- 宽度：设计了一种高效表达特征的稀疏性结构——Inception模块。

					- Inception v1网络结构

					- 1.使用1*1卷积进行升降维；2.在多个尺寸上同时进行卷积再聚合(concat)。

	- ResNet(2015，152层)

		- ILSVRC2015图像识别、图像检测、图像定位等多项冠军。

			- 背景

				- 在使用标准初始化和中间层正规化(如Batch Normalization)有效控制梯度弥散后，即使得模型训练可以收敛的前提下，引出了另一个问题——网络退化问题。
				- 网络退化问题：在神经网络可以收敛的前提下，随着网络深度增加，网络的表现先是逐渐增加至饱和，然后迅速下降。

					- 试验结果如下图所示，这并非是一个过拟合问题，56-layer的训练集和测试集误差都比20-layer高。
				- 试验的结果并不符合常理：若某个K层的网络f是当前最优的网络，那么总可以构造一个更深的网络，其最后基层仅是该网络f第K层输出的恒等映射(Identity Mapping)，就可以取得一致的结果；也许K还不是所谓"最佳层数"，那么更深的网络就可以取得更好的效果。也就是说，与浅层网络相比，更深的网络的表现不应该更差。因此，得到一个合理的猜测，对于神经网络来说，恒等映射函数并不容易拟合。

			- 核心思想

				- 既然神经网络不容易拟合一个恒等映射，那么我们就人为的构造天然的恒等映射。
				- 将恒等映射函数H(x)=x变形为残差函数F(x)=H(x)-x，只要F(x)=0就等价于构成了一个恒等映射H(x)=x。而且，拟合残差F(x)=0或F(x)-->0更加容易。
				- 残差块结构

					- F(x)是残差输出，x是用shortcut/跳跃连接实现的一个恒等映射，直接将F(x)和x相加在一起，再做ReLU激活。

				- 网络结构

					- 对于残差网络，维度匹配的shortcut连接为实线，反之为虚线。

						- 维度匹配时，H(x)=F(x)+x
						- 维度不匹配时，同等映射有两种可选方案：

							- 1）直接通过zero padding来增加维度(channel)。
							- 2）乘以W矩阵投影到新的空间，H(x)=F(x)+Wx。实现是用1x1卷积实现的，直接改变1x1卷积的filters数目。这种会增加参数。

					- 深层探究

						- 考虑到时间花费，将原来的building block(残差学习结构)改为瓶颈结构，首端和末端的1x1卷积用来削减和恢复维度。减少参数量/计算量的同时增加了网络的深度。

			- 残差解决了什么问题？

				- 尽管残差网络的提出是为了解决网络退化问题，但关于其作用机制，还是多有争议。
				- 1）在前向传播时，输入信号可以从任意低层直接传播到高层。由于包含了一个天然的恒等映射，一定程度上可以解决网络退化问题。
				- 2）损失对某层的梯度被分解为两项，一定程度上可以缓解梯度弥散问题(即便中间层矩阵权重很小，梯度也基本不会消失)。

	- DenseNet(2017，)

		- CVPR 2017最佳论文

			- 设计理念：相比ResNet，提出了一个更激进的密集连接方式——通过建立前面所有层与后面层的密集连接(dense connection)，并通过channel上的concat实现特征重用(feature reuse)。
			- 结构

- 5）通道优化

	- 1.优化操作

		- 前提

			- 标准卷积

				- 空间/像素层面：局部连接；通道层面：全连接。
			- 如何进一步优化网络，降低参数量和运算成本？

		- 1）分组卷积(Group convolution)

			- 结构

			- 分组卷积最早出现在AlexNet中。在CNN发展初期，GPU资源不足以满足训练任务的要求，因此Hinton采用了多GPU训练的策略，每个GPU完成一部分卷积，最后把多个GPU的卷积结果进行融合。
			- 分组卷积的参数量是标准卷积的1/g。

				- 标准卷积: 输出特征图上的每一个点，是由输入特征图h1*w1*c1个点计算得到；
				- 分组卷积: 输出特征图上的每一个点，是由输入特征图h1*w1*(c1/g)个点计算得到。

		- 2）深度可分离卷积(Depthwise Separable Convolution)

			- 将标准卷积分解为两个过程

				- 第一个过程: Depthwise Convolution(深度卷积)

					- 将(N,C,H,W)分为group=C组，一个卷积核负责一个通道，一个通道只被一个卷积核卷积。
					- 像素融合。
					- Depthwise Convolution后的Feature map数量与输入通道数一致，无法扩展Feature map数量；而且这种操作每个通道的卷积运算是独立的，缺少通道间的信息交流。因此需要Pointwise Convolution来将这些Feature map进行组合生成新的Feature map。

				- 第二个过程: Pointwise Convolution(点卷积)

					- 通道融合，并可进行升降维操作。

			- 该结构和常规卷积操作类似，可用来提取特征，但相比于常规卷积操作，其参数量和运算成本较低。所以在一些轻量级网络中会碰到这种结构，如MobileNet、ShuffleNet。
			- 优缺点

				- 优点：减小参数量和计算量，降低了对部署设备的算力要求。
				- 缺点：通道局部连接，会丢失部分特征。

		- 3）通道混洗

			- 通道进行分组后，通道间的交互性差，在不改变通道数的情况下，可采用"通道混洗"来提升通道间的融合能力。
		- 4）卷积分解

			- 将3x3卷积分解成1x3和3x1两个卷积。
			- 可以压缩模型参数量（这里参数由3x3=9降低到1x3+3x1=6），但是计算量基本没变。

	- 其他

		- 在PyTorch框架中

			- _ConvNd初始化，self.weight = Parameter(torch.Tensor(out_channels, in_channels // groups, *kernel_size))
			- 即卷积核的数量不变，但每个卷积核的通道数变为in_channels // groups。
			- 注意：设置group时，in_channels和out_channels必须能被groups整除。

		- thop第三方库

			- 1）统计模型的计算量ops和参数量params

				- total_ops, total_params = thop.profile(model=conv1, inputs=(x,))

			- 2）格式转换(提升输出结果的可读性)

				- clever_nums = thop.clever_format(nums=[total_ops, total_params], format="%.2f")

			- 注意：如果要统计nn.Conv2d(6, 30, 3, 1, groups=2)的计算量，必须将其放到nn.Module内部才可以。

		- tensorboard

			- 显示权重weight直方图

				- summaryWriter.add_histogram("layer1", layer1_weight, epoch)

			- 显示图片

				- summaryWriter.add_images("images", img, epoch)

	- 2.一些优化的网络

		- 残差网络

			- ResNet(Bottleneck)

				- 残差块的瓶颈结构，首尾的1*1核-->升维、降维，3*3核-->像素融合、特征提取。
				- 这样设计可以在减少参数量和计算量的同时，让网络模型变得更深。

			- ResNeXt

				- 论文

					- 《Aggregated Residual Transformations for Deep Neural Networks》
					- 下载地址：https://arxiv.org/abs/1611.05431

				- 背景

					- 1）传统的要提高模型的准确率，都是加深或加宽网络，但是随着超参数数量的增加（比如channels数，filter size等等），网络设计的难度和计算开销也会增加。
					- 2）Inception系列网络有个问题：网络的超参数设定的针对性比较强，当应用在别的数据集上时需要修改许多参数，因此可扩展性一般。

				- 思想

					- 借鉴了VGG堆叠的思想(之前ResNet也借用这种思想)和Inception的split-transform-merge思想，作者提出ResNeXt 结构，可以在不增加准确率的同时基本不改变或降低模型的复杂度，可扩展性也比较强。
					- 提到一个名词cardinality(实际上是一个Group的概念)，经过试验表明: 增加 cardinality 比增加深度和宽度更有效。

				- 结构

		- 轻量级网络

			- 近来，深度CNN网络如ResNet和DenseNet，已经极大地提高了图像分类的准确度。但是除了准确度外，计算复杂度也是CNN网络要考虑的重要指标，过复杂的网络可能速度很慢，一些特定场景如无人车领域需要低延迟。另外移动端设备也需要既准确又快的小模型。为了满足这些需求，一些轻量级的CNN网络如MobileNet和ShuffleNet被提出，它们在速度和准确度之间做了很好地平衡。
			- MobileNetV1

				- 结构

					- 一个3*3核只做像素融合，一个1*1核只做通道融合。

				- 优缺点

					- 优点：每层只做一种融合操作，参数量和计算量特别少，速度快。
					- 缺点：精度差。

			- MobileNetV2

				- 结构

					- 对MobileNetV1的改进，先用1*1进行升维，再用3*3进行像素融合，最后用1*1进行降维。

				- 与MobileNetV1一样，每层只做一种融合操作(3*3像素融合或1*1通道融合)，速度也很快。相比于MobileNetV1，通过升维增加模型的参数量，从而提高模型的精度。当然，精度仍然比Resnet弱。

			- ShuffleNetV1

				- 结构

					- 1*1组卷积+通道混洗

				- 和MobileNet一样，每层只做一种融合操作。更进一步，在Pointwise Convolution(通道融合)上引入分组来减少参数量和计算量；同时通过对不同的channels进行shuffle来解决Group Convolution带来的弊端。

			- ShuffleNetV2

				- 设计理念

					- FLOPs(浮点运算量/计算量)是衡量模型复杂度的一个通用指标，具体指的是multiply-add数量，但是这却是一个间接指标，因为它不完全等同于速度。相同FLOPs的两个模型，速度间也是存在差异的，影响速度的不仅仅是FLOPs，内存的使用量(memory access cost, MAC)、模型的并行程度也会影响速度。另外，模型在不同的平台上，以及采用不同的库也会有影响。
					- 作者根据4条实用的指导原则，对ShuffleNetV1进一步优化：

						- G1）同等通道大小最小化内存访问量。
						- G2）过量使用组卷积会增加MAC(内存使用量)。
						- G3）网络碎片化会降低并行度。
						- G4）不能忽略元素级操作。

				- 结构

					- 左侧是ShuffleNetV1

						- 1）采用了类似ResNet的bottleneck结构，输入和输出通道数不同，这违背了G1原则。
						- 2）大量使用了1*1组卷积，违背了G2原则。
						- 3）同时使用过多的组，也违背了G3原则。
						- 4）shortcut中存在大量的元素级Add运算，违背了G4原则。

					- 右侧是ShuffleNetV2

						- 一开始先将输入特征图在通道维度上分成两个分支：通道数分别为c'和c-c'，实际上c'=c/2。
						- 1）左边分支做同等映射，右边的分支包含3个连续的卷积，并且输入和输出通道相同，这符合G1。
						- 2）而且两个1x1卷积不再是组卷积，这符合G2，另外两个分支相当于已经分成两组。
						- 3）两个分支的输出不再是Add元素，而是concat在一起，符合原则G4。

				- ShuffleNetV2是轻量级CNN网络中的桂冠，在同等复杂度(计算量)下，ShuffleNetV2比ShuffleNetV1和MobileNetV2更快、更准确。

		- Inception系列

			- GoogLeNet

		- SqueezeNet

